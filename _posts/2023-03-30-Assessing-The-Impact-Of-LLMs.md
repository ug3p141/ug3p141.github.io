# Assessing the Impact of Large Language Models

![Created using Midjourney](/assets/human_robot_look_at_each_other.png)

## Introduction
Since the launch of ChatGPT large language models have taken the world by storm and each enterprise is reevaluating 
its AI strategy and wondering what LLMs generally and commercial models like ChatGPT and Bard especially 
will mean for their business. In this piece we're trying to look beyond the current hype surrounding the latest models
and look into the midterm and longterm future of NLP techniques and related areas to give some orientation or more humbly 
hints into some directions to ponder which hopefully will survive the headlines of today.
To this end we broaden the perspective with highlighting the historical trajectory from which all this started and even get
into somewhat philosophical territory in the hope to get a glimpse of where all this stuff is headed, what has been achieved, what is missing,
what is to be expected.

## History
- **Late 1980s to mid 1990s**: The foundations of neural networks and basic language models are developed.

- **2001**: Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, and Christian Janvin published a paper on a neural probabilistic language model, a significant early work in using neural networks for language processing.

- **2003**: Bengio and his team continued to improve upon their initial models, which helped spark broader interest in the field.

- **2013**: Word2Vec, developed by Tomas Mikolov and his team at Google, made a significant impact by effectively converting words into vectors, which could then be processed by neural networks in a meaningful way.

- **2014**: Sequence to Sequence Learning was introduced by Ilya Sutskever, Oriol Vinyals, and Quoc Le. This was a major advancement, as it led to the development of models that could generate more coherent and meaningful sequences of text.

- **2015**: Attention Mechanism was introduced, making it possible for models to learn to focus on different parts of the input sequence when generating output. This improved the ability of models to handle long sequences of text.

- **2018**: Transformer architecture was introduced in the paper "Attention is All You Need" by Vaswani et al. It was a key development because it enabled much more effective training of large language models.

- **2018**: BERT (Bidirectional Encoder Representations from Transformers) was developed by Google. BERT was significant because it was trained to predict missing words in a sentence, rather than just predicting the next word in a sequence.

- **2019**: GPT-2, developed by OpenAI, made a significant leap in the size of models and their ability to generate surprisingly human-like text. GPT-2 was also notable because of the decision by OpenAI to initially not release the full model due to concerns about potential misuse.

- **2020**: GPT-3 was released by OpenAI. With 175 billion parameters, GPT-3 was an order of magnitude larger than any previous model, and its capabilities reflected that size increase.

- **2021**: Large language models started to see wide deployment in commercial applications, such as customer service, content generation, tutoring, and more.

- **early 2022**: GPT-3.5 was released by OpenAI. These models were described as more capable than previous versions and were trained on data up to June 2021.

- **late 2022**: ChatGPT was released by OpenAI, a model fine-tuned to target conversational usage by a process called reinforcement learning from human feedback (RLHF).

- **early 2023**: a plethora of both commercial models and open source versions of LLMs come force to counter the dominance of ChatGPT

## Economic and Scientific Environment

### Prerequisites and Conditions for Building, Maintaining and Operating LLMs

### Scientific Environment

### LLM Economy

## Further Development

### Current Status

### The GPT-H Hypothesis

The capabilities of LLMs in turn-dialogue talking, reasoning and instruction following have reached an impressive maturity. These capabilities were discovered by scaling up the models and training them on ever larger text corpora and are therefore referred to as emergent phenomena. It is important to emphasize that they are not built into the models by special training techniques, in fact techniques to bring about these skills are not known. The only technique built into the models and trained is next word prediction.

### Gaps and Features
- Hallucinations
- Adversarial Prompting
- Missing Reference to Ground Truth
- QA

#### Access to UpToDate Knowledge

#### Continuous Learning

#### Tool Use

#### Long Term Memory

### Long Term Development

#### Integrated AI

#### Generative AI

#### Self Referential AI

#### Physical Access

## Impacts

### Immediate and Near Term Impacts

#### End of SE Domination

#### Document Creation

#### Code Generation

### Mid Term Impacts

#### Front End to Everything

#### Computer Program Generation

#### Use of LLMs within Programs

### Long Term Impacts

#### Self Referential AI And Exponential Acceleration

#### Knowledge Creation and Science

#### Creative Industry

#### Engineering

### Economic Impacts

#### Rise of Mega Monopolies

#### Industry Domination